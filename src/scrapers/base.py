import logging
import asyncio
import re
import time
from abc import ABC, abstractmethod
from typing import Any, Callable, Dict, List, Optional, Type, Tuple, TYPE_CHECKING
from typing import Union
from functools import wraps
import httpx
from sqlalchemy.ext.asyncio import AsyncSession, async_sessionmaker

from src.db import crud
from src.db import models
from src.core.cache import get_cache_backend

from src.utils import TransportManager

if TYPE_CHECKING:
    from src.db import ConfigManager

def _roman_to_int(s: str) -> int:
    """å°†ç½—é©¬æ•°å­—å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ•´æ•°ã€‚"""
    roman_map = {'I': 1, 'V': 5, 'X': 10, 'L': 50, 'C': 100, 'D': 500, 'M': 1000}
    s = s.upper()
    result = 0
    i = 0
    while i < len(s):
        # å¤„ç†å‡æ³•è§„åˆ™ (e.g., IV, IX)
        if i + 1 < len(s) and roman_map[s[i]] < roman_map[s[i+1]]:
            result += roman_map[s[i+1]] - roman_map[s[i]]
            i += 2
        else:
            result += roman_map[s[i]]
            i += 1
    return result

def get_season_from_title(title: str) -> int:
    """ä»æ ‡é¢˜ä¸­è§£æå­£åº¦ä¿¡æ¯ï¼Œè¿”å›å­£åº¦æ•°ã€‚"""
    if not title:
        return 1

    # A map for Chinese numerals, including formal and simple.
    chinese_num_map = {
        'ä¸€': 1, 'äºŒ': 2, 'ä¸‰': 3, 'å››': 4, 'äº”': 5, 'å…­': 6, 'ä¸ƒ': 7, 'å…«': 8, 'ä¹': 9, 'å': 10,
        'å£¹': 1, 'è´°': 2, 'å': 3, 'è‚†': 4, 'ä¼': 5, 'é™†': 6, 'æŸ’': 7, 'æŒ': 8, 'ç–': 9, 'æ‹¾': 10
    }

    # æ¨¡å¼çš„é¡ºåºå¾ˆé‡è¦
    patterns = [
        # æ ¼å¼: S01, Season 1
        (re.compile(r"(?:S|Season)\s*(\d+)", re.I), lambda m: int(m.group(1))),
        # æ ¼å¼: ç¬¬ X å­£/éƒ¨/å¹• (æ”¯æŒä¸­æ–‡å’Œé˜¿æ‹‰ä¼¯æ•°å­—)
        (re.compile(r"ç¬¬\s*([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åå£¹è´°åè‚†ä¼é™†æŸ’æŒç–æ‹¾\d])\s*[å­£éƒ¨å¹•]", re.I),
         lambda m: chinese_num_map.get(m.group(1)) if not m.group(1).isdigit() else int(m.group(1))),
        # æ ¼å¼: Xä¹‹ç«  (æ”¯æŒç®€ç¹ä¸­æ–‡æ•°å­—)
        (re.compile(r"([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åå£¹è´°åè‚†ä¼é™†æŸ’æŒç–æ‹¾])\s*ä¹‹\s*ç« ", re.I),
         lambda m: chinese_num_map.get(m.group(1))),
        # æ ¼å¼: Unicode ç½—é©¬æ•°å­—, e.g., â…¢
        (re.compile(r"\s+([â… -â…«])(?=\s|$)", re.I),
         lambda m: {'â… ': 1, 'â…¡': 2, 'â…¢': 3, 'â…£': 4, 'â…¤': 5, 'â…¥': 6, 'â…¦': 7, 'â…§': 8, 'â…¨': 9, 'â…©': 10, 'â…ª': 11, 'â…«': 12}.get(m.group(1).upper())),
        # æ ¼å¼: ASCII ç½—é©¬æ•°å­—, e.g., III
        (re.compile(r"\s+([IVXLCDM]+)\b", re.I), lambda m: _roman_to_int(m.group(1))),
        # æ ¼å¼: æ ‡é¢˜æœ«å°¾çš„é˜¿æ‹‰ä¼¯æ•°å­—, e.g., åˆ€å‰‘ç¥åŸŸ2, æ¨¡èŒƒå‡ºç§Ÿè½¦3
        # åŒ¹é…éæ•°å­—å­—ç¬¦åè·Ÿ1-2ä½æ•°å­—ç»“å°¾ï¼Œæ’é™¤å¹´ä»½(4ä½æ•°å­—)
        (re.compile(r"[^\d](\d{1,2})\s*$"), lambda m: int(m.group(1)) if 1 <= int(m.group(1)) <= 20 else None),
    ]

    for pattern, handler in patterns:
        match = pattern.search(title)
        if match:
            try:
                season = handler(match)
                if season is not None: return season
            except (ValueError, KeyError, IndexError):
                continue
    return 1 # Default to season 1


def track_performance(func):
    """
    è£…é¥°å™¨: è·Ÿè¸ªå¼‚æ­¥æ–¹æ³•çš„æ‰§è¡Œæ—¶é—´,ä¸å½±å“å¹¶å‘æ€§èƒ½ã€‚
    è®°å½•åˆ° INFO çº§åˆ«,æ–¹ä¾¿æŸ¥çœ‹æ€§èƒ½ç»Ÿè®¡ã€‚
    ä½¿ç”¨ä»»åŠ¡IDä½œä¸ºé”®å­˜å‚¨è€—æ—¶ï¼Œç¡®ä¿å¹¶å‘å®‰å…¨ï¼Œä¾› scraper_manager è¯»å–ã€‚
    """
    @wraps(func)
    async def wrapper(self, *args, **kwargs):
        start_time = time.perf_counter()
        task_id = id(asyncio.current_task())  # è·å–å½“å‰ä»»åŠ¡IDï¼Œç¡®ä¿å¹¶å‘å®‰å…¨
        try:
            result = await func(self, *args, **kwargs)
            elapsed = time.perf_counter() - start_time
            elapsed_ms = elapsed * 1000
            # ä½¿ç”¨ä»»åŠ¡IDä½œä¸ºé”®å­˜å‚¨è€—æ—¶ï¼Œç¡®ä¿å¹¶å‘å®‰å…¨
            if not hasattr(self, '_task_timings'):
                self._task_timings = {}
            self._task_timings[task_id] = elapsed_ms
            # è®°å½•åˆ° INFO çº§åˆ«,æ˜¾ç¤ºæœç´¢æºåç§°å’Œè€—æ—¶
            self.logger.info(f"[{self.provider_name}] {func.__name__} è€—æ—¶: {elapsed:.3f}s")
            return result
        except Exception as e:
            elapsed = time.perf_counter() - start_time
            elapsed_ms = elapsed * 1000
            # å³ä½¿å¤±è´¥ä¹Ÿå­˜å‚¨è€—æ—¶
            if not hasattr(self, '_task_timings'):
                self._task_timings = {}
            self._task_timings[task_id] = elapsed_ms
            self.logger.warning(f"[{self.provider_name}] {func.__name__} å¤±è´¥è€—æ—¶: {elapsed:.3f}s")
            raise
    return wrapper


# é€šç”¨åˆ†é›†è¿‡æ»¤è§„åˆ™ï¼ˆç¡¬ç¼–ç ï¼‰ï¼Œç”¨äºå‰ç«¯"å¡«å……é€šç”¨è§„åˆ™"æŒ‰é’®
COMMON_EPISODE_BLACKLIST_REGEX = r'^(.*?)((.+?ç‰ˆ)|(ç‰¹(åˆ«|å…¸))|((å¯¼|æ¼”)å‘˜|å˜‰å®¾|è§’è‰²)è®¿è°ˆ|ç¦åˆ©|å½©è›‹|èŠ±çµ®|é¢„å‘Š|ç‰¹è¾‘|ä¸“è®¿|è®¿è°ˆ|å¹•å|å‘¨è¾¹|èµ„è®¯|çœ‹ç‚¹|é€Ÿçœ‹|å›é¡¾|ç›˜ç‚¹|åˆé›†|PV|MV|CM|OST|ED|OP|BD|ç‰¹å…¸|SP|NCOP|NCED|MENU|Web-DL|rip|x264|x265|aac|flac)(.*?)$'


class BaseScraper(ABC):
    """
    æ‰€æœ‰æœç´¢æºçš„æŠ½è±¡åŸºç±»ã€‚
    å®šä¹‰äº†æœç´¢åª’ä½“ã€è·å–åˆ†é›†å’Œè·å–å¼¹å¹•çš„é€šç”¨æ¥å£ã€‚

    æ³¨æ„ï¼šåˆ†é›†è¿‡æ»¤è§„åˆ™ç°åœ¨å®Œå…¨ä» config è¡¨è¯»å–ï¼Œä¸å†ä½¿ç”¨ç¡¬ç¼–ç çš„é»˜è®¤å€¼ã€‚
    - ç‰¹å®šæºåˆ†é›†é»‘åå•ï¼š{provider_name}_episode_blacklist_regex
    å¦‚æœ config è¡¨ä¸­é”®ä¸å­˜åœ¨ï¼Œå¯åŠ¨æ—¶ä¼šé€šè¿‡ register_defaults åˆ›å»ºå¹¶å¡«å……é»˜è®¤å€¼ã€‚
    å¦‚æœé”®å­˜åœ¨ä½†å€¼ä¸ºç©ºï¼Œåˆ™ä¸è¿›è¡Œè¿‡æ»¤ã€‚
    """

    def __init__(self, session_factory: async_sessionmaker[AsyncSession], config_manager: "ConfigManager", transport_manager: TransportManager):
        self._session_factory = session_factory
        self.config_manager = config_manager
        self.transport_manager = transport_manager
        self.logger = logging.getLogger(self.__class__.__name__)
        # ç”¨äºè·Ÿè¸ªå½“å‰å®¢æˆ·ç«¯å®ä¾‹æ‰€ä½¿ç”¨çš„ä»£ç†é…ç½®
        self._current_proxy_config: Optional[str] = None
        # ç¼“å­˜ scraper_manager å¼•ç”¨,ç”¨äºè®¿é—®é¢„åŠ è½½çš„ scraper è®¾ç½®
        self._scraper_manager_ref: Optional[Any] = None

    async def _get_proxy_for_provider(self) -> Optional[str]:
        """
        è·å–å½“å‰ provider çš„ä»£ç†é…ç½®ã€‚
        ä¼˜å…ˆä½¿ç”¨é¢„åŠ è½½çš„ç¼“å­˜,é¿å…é‡å¤æ•°æ®åº“æŸ¥è¯¢ã€‚

        æ”¯æŒä¸‰ç§ä»£ç†æ¨¡å¼ï¼š
        - none: ä¸ä½¿ç”¨ä»£ç†
        - http_socks: HTTP/SOCKS ä»£ç†
        - accelerate: åŠ é€Ÿä»£ç†ï¼ˆURL é‡å†™æ¨¡å¼ï¼Œä¸è¿”å›ä»£ç† URLï¼‰
        """
        # è·å–ä»£ç†æ¨¡å¼
        proxy_mode = await self.config_manager.get("proxyMode", "none")

        # å…¼å®¹æ—§é…ç½®ï¼šå¦‚æœ proxyMode ä¸º none ä½† proxyEnabled ä¸º trueï¼Œåˆ™ä½¿ç”¨ http_socks æ¨¡å¼
        if proxy_mode == "none":
            proxy_enabled_globally = (await self.config_manager.get("proxyEnabled", "false")).lower() == 'true'
            if proxy_enabled_globally:
                proxy_mode = "http_socks"

        # å¦‚æœä»£ç†æ¨¡å¼ä¸º none æˆ– accelerateï¼Œåˆ™ä¸è¿”å› HTTP ä»£ç† URL
        # accelerate æ¨¡å¼é€šè¿‡ URL é‡å†™å®ç°ï¼Œä¸éœ€è¦è®¾ç½® httpx çš„ proxy å‚æ•°
        if proxy_mode != "http_socks":
            return None

        proxy_url = await self.config_manager.get("proxyUrl", "")
        if not proxy_url:
            return None

        # è·å–å½“å‰ provider çš„ä»£ç†è®¾ç½®
        provider_setting = None
        if self._scraper_manager_ref and hasattr(self._scraper_manager_ref, '_cached_scraper_settings'):
            # ä½¿ç”¨é¢„åŠ è½½çš„ç¼“å­˜ï¼ˆå¿«é€Ÿè·¯å¾„ï¼‰
            provider_setting = self._scraper_manager_ref._cached_scraper_settings.get(self.provider_name)
        else:
            # é™çº§åˆ°æ•°æ®åº“æŸ¥è¯¢ï¼ˆä»…åœ¨ç¼“å­˜æœªåˆå§‹åŒ–æ—¶ï¼Œå¦‚æµ‹è¯•ç¯å¢ƒï¼‰
            async with self._session_factory() as session:
                scraper_settings = await crud.get_all_scraper_settings(session)
            provider_setting = next((s for s in scraper_settings if s['providerName'] == self.provider_name), None)

        use_proxy_for_this_provider = provider_setting.get('useProxy', False) if provider_setting else False

        return proxy_url if use_proxy_for_this_provider else None

    async def _should_use_accelerate_proxy(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥ä½¿ç”¨åŠ é€Ÿä»£ç†æ¨¡å¼"""
        proxy_mode = await self.config_manager.get("proxyMode", "none")
        return proxy_mode == "accelerate"

    async def _get_accelerate_proxy_url(self) -> str:
        """è·å–åŠ é€Ÿä»£ç†åœ°å€"""
        return await self.config_manager.get("accelerateProxyUrl", "")

    def _transform_url_for_accelerate(self, original_url: str, proxy_base: str) -> str:
        """
        è½¬æ¢ URL ä¸ºåŠ é€Ÿä»£ç†æ ¼å¼

        åŸå§‹: https://api.example.com/path
        è½¬æ¢: https://proxy.vercel.app/https/api.example.com/path
        """
        if not proxy_base:
            return original_url

        proxy_base = proxy_base.rstrip('/')
        protocol = "https" if original_url.startswith("https://") else "http"
        target = original_url.replace(f"{protocol}://", "")

        return f"{proxy_base}/{protocol}/{target}"

    async def _transform_url_if_needed(self, url: str) -> str:
        """
        æ ¹æ®ä»£ç†æ¨¡å¼è½¬æ¢ URL

        - none/http_socks: è¿”å›åŸå§‹ URL
        - accelerate: è¿”å›åŠ é€Ÿä»£ç†æ ¼å¼çš„ URLï¼ˆå¦‚æœå½“å‰ provider å¯ç”¨äº†ä»£ç†ï¼‰
        """
        if not await self._should_use_accelerate_proxy():
            return url

        # æ£€æŸ¥å½“å‰ provider æ˜¯å¦å¯ç”¨äº†ä»£ç†
        provider_setting = None
        if self._scraper_manager_ref and hasattr(self._scraper_manager_ref, '_cached_scraper_settings'):
            provider_setting = self._scraper_manager_ref._cached_scraper_settings.get(self.provider_name)
        else:
            async with self._session_factory() as session:
                scraper_settings = await crud.get_all_scraper_settings(session)
            provider_setting = next((s for s in scraper_settings if s['providerName'] == self.provider_name), None)

        use_proxy_for_this_provider = provider_setting.get('useProxy', False) if provider_setting else False

        if not use_proxy_for_this_provider:
            return url

        proxy_base = await self._get_accelerate_proxy_url()
        if proxy_base:
            return self._transform_url_for_accelerate(url, proxy_base)

        return url
    
    async def _log_proxy_usage(self, proxy_url: Optional[str]):
        if proxy_url:
            self.logger.debug(f"é€šè¿‡ä»£ç† '{proxy_url}' å‘èµ·è¯·æ±‚...")

    async def _create_client(self, **kwargs) -> httpx.AsyncClient: # type: ignore
        """
        åˆ›å»º httpx.AsyncClientï¼Œå¹¶æ ¹æ®é…ç½®åº”ç”¨ä»£ç†ã€‚
        å­ç±»å¯ä»¥ä¼ é€’é¢å¤–çš„ httpx.AsyncClient å‚æ•°ã€‚
        """
        proxy_to_use = await self._get_proxy_for_provider()
        await self._log_proxy_usage(proxy_to_use)
        self._current_proxy_config = proxy_to_use

        client_kwargs = {"proxy": proxy_to_use, "timeout": 20.0, "follow_redirects": True, **kwargs}
        return httpx.AsyncClient(**client_kwargs)

    async def _get_from_cache(self, key: str) -> Optional[Any]:
        """
        ä»ç¼“å­˜ä¸­è·å–æ•°æ®ã€‚
        ä¼˜å…ˆä½¿ç”¨é¢„å–çš„ç¼“å­˜ï¼ˆæ‰¹é‡æŸ¥è¯¢ä¼˜åŒ–ï¼‰ï¼Œå¦åˆ™å•ç‹¬æŸ¥è¯¢æ•°æ®åº“ã€‚
        """
        # ã€ä¼˜åŒ–ã€‘ä¼˜å…ˆä½¿ç”¨é¢„å–çš„ç¼“å­˜
        if hasattr(self, '_prefetched_cache'):
            if key in self._prefetched_cache:
                cached_value = self._prefetched_cache[key]
                if cached_value is not None:
                    self.logger.debug(f"{self.provider_name}: ä½¿ç”¨é¢„å–ç¼“å­˜ (å‘½ä¸­) - {key}")
                    return cached_value
                else:
                    # æ‰¹é‡æŸ¥è¯¢å·²æ‰§è¡Œï¼Œä½†ç¼“å­˜ä¸å­˜åœ¨
                    self.logger.debug(f"{self.provider_name}: ä½¿ç”¨é¢„å–ç¼“å­˜ (æœªå‘½ä¸­) - {key}")
                    return None
        
        # é™çº§åˆ°å•ç‹¬æ•°æ®åº“æŸ¥è¯¢ï¼ˆä»…åœ¨æ‰¹é‡æŸ¥è¯¢æœªæ‰§è¡Œæ—¶ï¼‰
        self.logger.debug(f"{self.provider_name}: ç¼“å­˜æœªé¢„å–ï¼Œè¿›è¡Œå•ç‹¬æŸ¥è¯¢ - {key}")
        async with self._session_factory() as session:
            try:
                _backend = get_cache_backend()
                if _backend is not None:
                    try:
                        result = await _backend.get(key, region="default")
                        if result is not None:
                            return result
                    except Exception:
                        pass
                return await crud.get_cache(session, key)
            finally:
                await session.close()

    async def _set_to_cache(self, key: str, value: Any, config_key: str, default_ttl: int):
        """å°†æ•°æ®å­˜å…¥æ•°æ®åº“ç¼“å­˜ï¼ŒTTLä»é…ç½®ä¸­è¯»å–ã€‚"""
        ttl_str = await self.config_manager.get(config_key, str(default_ttl))
        ttl = int(ttl_str)
        if ttl > 0:
            async with self._session_factory() as session:
                try:
                    _backend = get_cache_backend()
                    if _backend is not None:
                        try:
                            await _backend.set(key, value, ttl=ttl, region="default")
                        except Exception:
                            await crud.set_cache(session, key, value, ttl, provider=self.provider_name)
                    else:
                        await crud.set_cache(session, key, value, ttl, provider=self.provider_name)
                    await session.commit()
                finally:
                    await session.close()

    # æ¯ä¸ªå­ç±»éƒ½å¿…é¡»è¦†ç›–è¿™ä¸ªç±»å±æ€§
    provider_name: str

    # (å¯é€‰) å­ç±»å¯ä»¥è¦†ç›–æ­¤å­—å…¸æ¥å£°æ˜å…¶å¯é…ç½®çš„å­—æ®µã€‚
    # æ ¼å¼: { "config_key": ("UIæ˜¾ç¤ºçš„æ ‡ç­¾", "å­—æ®µç±»å‹", "UIä¸Šçš„æç¤ºä¿¡æ¯") }
    # æ”¯æŒçš„å­—æ®µç±»å‹: "string", "boolean", "password"
    configurable_fields: Dict[str, Tuple[str, str, str]] = {}

    # (æ–°å¢) å­ç±»åº”è¦†ç›–æ­¤åˆ—è¡¨ï¼Œå£°æ˜å®ƒä»¬å¯ä»¥å¤„ç†çš„åŸŸå
    handled_domains: List[str] = []

    # (æ–°å¢) å­ç±»å¯ä»¥è¦†ç›–æ­¤å±æ€§ï¼Œä»¥æä¾›ä¸€ä¸ªé»˜è®¤çš„ Referer
    referer: Optional[str] = None

    # (æ–°å¢) å­ç±»å¯ä»¥è¦†ç›–æ­¤å±æ€§ï¼Œä»¥è¡¨æ˜å…¶æ˜¯å¦æ”¯æŒæ—¥å¿—è®°å½•
    is_loggable: bool = True

    rate_limit_quota: Optional[int] = None # æ–°å¢ï¼šç‰¹å®šæºçš„é…é¢

    # ç‚¹èµç«ç„°é˜ˆå€¼ï¼šl >= æ­¤å€¼æ˜¾ç¤º ğŸ”¥ï¼Œå¦åˆ™æ˜¾ç¤º â¤ï¸ï¼ˆå„æºå¯åœ¨å†…éƒ¨è¦†ç›–ï¼‰
    likes_fire_threshold: int = 1000

    def build_media_url(self, media_id: str) -> Optional[str]:
        """
        æ„é€ å¹³å°æ’­æ”¾é¡µé¢URLã€‚
        å­ç±»å¯ä»¥è¦†ç›–æ­¤æ–¹æ³•ä»¥æä¾›ç‰¹å®šå¹³å°çš„URLæ„é€ é€»è¾‘ã€‚

        Args:
            media_id: åª’ä½“ID

        Returns:
            å¹³å°æ’­æ”¾é¡µé¢URLï¼Œå¦‚æœæ— æ³•æ„é€ åˆ™è¿”å›None
        """
        return None
    
    async def _should_log_responses(self) -> bool:
        """åŠ¨æ€æ£€æŸ¥æ˜¯å¦åº”è®°å½•åŸå§‹å“åº”ï¼Œç¡®ä¿é…ç½®å®æ—¶ç”Ÿæ•ˆã€‚"""
        if not self.is_loggable:
            return False

        # ä¿®æ­£ï¼šä½¿ç”¨ç‰¹å®šäºæä¾›å•†çš„é…ç½®é”®ï¼Œä¾‹å¦‚ 'scraper_tencent_log_responses'
        config_key = f"scraper_{self.provider_name}_log_responses"
        is_enabled_str = await self.config_manager.get(config_key, "false")
        # å¥å£®æ€§æ£€æŸ¥ï¼šåŒæ—¶å¤„ç†å¸ƒå°”å€¼å’Œå­—ç¬¦ä¸² "true"ï¼Œä»¥é˜²é…ç½®å€¼ç±»å‹ä¸ç¡®å®šã€‚
        if isinstance(is_enabled_str, bool):
            return is_enabled_str
        return str(is_enabled_str).lower() == 'true'

    async def get_episode_blacklist_pattern(self) -> Optional[re.Pattern]:
        """
        è·å–ç”¨äºè¿‡æ»¤åˆ†é›†æ ‡é¢˜çš„æ­£åˆ™è¡¨è¾¾å¼å¯¹è±¡ã€‚
        åªä½¿ç”¨ç‰¹å®šäºæä¾›å•†çš„é»‘åå•ï¼Œä¸å†æœ‰å…¨å±€é»‘åå•ã€‚

        æ³¨æ„ï¼šæ­¤æ–¹æ³•ä¸ä½¿ç”¨ç¡¬ç¼–ç çš„é»˜è®¤å€¼ä½œä¸ºå…œåº•ã€‚
        - å¦‚æœ config è¡¨ä¸­é”®ä¸å­˜åœ¨ï¼Œå¯åŠ¨æ—¶ä¼šé€šè¿‡ register_defaults åˆ›å»ºå¹¶å¡«å……é»˜è®¤å€¼
        - å¦‚æœé”®å­˜åœ¨ä½†å€¼ä¸ºç©ºï¼Œåˆ™ä¸è¿›è¡Œè¿‡æ»¤
        """
        # è·å–ç‰¹å®šäºæä¾›å•†çš„é»‘åå•
        provider_key = f"{self.provider_name}_episode_blacklist_regex"
        # ä¸æä¾›é»˜è®¤å€¼ï¼Œå¦‚æœæ•°æ®åº“ä¸­æ²¡æœ‰åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²
        provider_pattern_str = await self.config_manager.get(provider_key, "")

        if not provider_pattern_str or not provider_pattern_str.strip():
            return None

        try:
            return re.compile(provider_pattern_str, re.IGNORECASE)
        except re.error as e:
            self.logger.error(f"ç¼–è¯‘åˆ†é›†é»‘åå•æ­£åˆ™è¡¨è¾¾å¼å¤±è´¥: '{provider_pattern_str}'. é”™è¯¯: {e}")
        return None

    async def execute_action(self, action_name: str, payload: Dict[str, Any]) -> Any:
        """
        æ‰§è¡Œä¸€ä¸ªæŒ‡å®šçš„æ“ä½œã€‚
        å­ç±»åº”é‡å†™æ­¤æ–¹æ³•æ¥å¤„ç†å…¶å£°æ˜çš„æ“ä½œã€‚
        :param action_name: è¦æ‰§è¡Œçš„æ“ä½œçš„åç§°ã€‚
        :param payload: åŒ…å«æ“ä½œæ‰€éœ€å‚æ•°çš„å­—å…¸ã€‚
        """
        raise NotImplementedError(f"æ“ä½œ '{action_name}' åœ¨ {self.provider_name} ä¸­æœªå®ç°ã€‚")

    @abstractmethod
    async def search(self, keyword: str, episode_info: Optional[Dict[str, Any]] = None) -> List[models.ProviderSearchInfo]:
        """
        æ ¹æ®å…³é”®è¯æœç´¢åª’ä½“ã€‚
        episode_info: å¯é€‰å­—å…¸ï¼ŒåŒ…å« 'season' å’Œ 'episode'ã€‚
        """
        raise NotImplementedError

    @abstractmethod
    async def get_info_from_url(self, url: str) -> Optional[models.ProviderSearchInfo]:
        """
        (æ–°å¢) ä»ä¸€ä¸ªä½œå“çš„URLä¸­æå–ä¿¡æ¯ï¼Œå¹¶è¿”å›ä¸€ä¸ª ProviderSearchInfo å¯¹è±¡ã€‚
        è¿™ç”¨äºæ”¯æŒä»URLç›´æ¥å¯¼å…¥æ•´ä¸ªä½œå“ã€‚
        """
        raise NotImplementedError

    @abstractmethod
    async def get_id_from_url(self, url: str) -> Optional[Union[str, Dict[str, str]]]:
        """
        (æ–°å¢) ç»Ÿä¸€çš„ä»URLè§£æIDçš„æ¥å£ã€‚
        å­ç±»åº”é‡å†™æ­¤æ–¹æ³•ä»¥æ”¯æŒä»URLç›´æ¥å¯¼å…¥ã€‚
        """
        raise NotImplementedError

    @abstractmethod
    async def get_episodes(self, media_id: str, target_episode_index: Optional[int] = None, db_media_type: Optional[str] = None) -> List[models.ProviderEpisodeInfo]:
        """
        è·å–ç»™å®šåª’ä½“IDçš„æ‰€æœ‰åˆ†é›†ã€‚
        å¦‚æœæä¾›äº† target_episode_indexï¼Œåˆ™å¯ä»¥ä¼˜åŒ–ä¸ºåªè·å–åˆ°è¯¥åˆ†é›†ä¸ºæ­¢ã€‚
        db_media_type: ä»æ•°æ®åº“ä¸­è¯»å–çš„åª’ä½“ç±»å‹ ('movie', 'tv_series')ï¼Œå¯ç”¨äºæŒ‡å¯¼åˆ®å‰Šç­–ç•¥ã€‚
        """
        raise NotImplementedError

    @abstractmethod
    async def get_comments(self, episode_id: str, progress_callback: Optional[Callable] = None) -> List[dict]:
        """
        è·å–ç»™å®šåˆ†é›†IDçš„æ‰€æœ‰å¼¹å¹•ã€‚
        è¿”å›çš„å­—å…¸åˆ—è¡¨åº”ä¸ crud.save_danmaku_for_episode çš„æœŸæœ›æ ¼å¼å…¼å®¹ã€‚
        """
        raise NotImplementedError

    def format_episode_id_for_comments(self, provider_episode_id: Any) -> str:
        """
        (æ–°å¢) å°† get_comments æ‰€éœ€çš„ episode_id æ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²ã€‚
        å¤§å¤šæ•°æºç›´æ¥è¿”å›å­—ç¬¦ä¸²ï¼Œä½†Bilibiliå’ŒMGTVéœ€è¦ç‰¹æ®Šå¤„ç†ã€‚
        """
        return str(provider_episode_id)

    async def _filter_junk_episodes(self, episodes: List["models.ProviderEpisodeInfo"]) -> List["models.ProviderEpisodeInfo"]:
        """
        è¿‡æ»¤æ‰åƒåœ¾åˆ†é›†ï¼ˆé¢„å‘Šã€èŠ±çµ®ç­‰ï¼‰

        æ³¨æ„ï¼šæ­¤æ–¹æ³•ç°åœ¨ä» config è¡¨è¯»å–è¿‡æ»¤è§„åˆ™ï¼Œä¸å†ä½¿ç”¨ç¡¬ç¼–ç çš„æ­£åˆ™è¡¨è¾¾å¼ã€‚
        å¦‚æœ config è¡¨ä¸­æ²¡æœ‰é…ç½®è¿‡æ»¤è§„åˆ™ï¼Œåˆ™ä¸è¿›è¡Œè¿‡æ»¤ã€‚
        """
        if not episodes:
            return episodes

        # ä» config è¡¨è·å–è¿‡æ»¤è§„åˆ™ï¼Œä¸ä½¿ç”¨ç¡¬ç¼–ç å…œåº•
        blacklist_pattern = await self.get_episode_blacklist_pattern()

        # å¦‚æœæ²¡æœ‰é…ç½®è¿‡æ»¤è§„åˆ™ï¼Œç›´æ¥è¿”å›æ‰€æœ‰åˆ†é›†
        if not blacklist_pattern:
            self.logger.info(f"{self.provider_name}: åˆ†é›†è¿‡æ»¤ç»“æœ (æ— è¿‡æ»¤è§„åˆ™): å…± {len(episodes)} é›†")
            return episodes

        filtered_episodes = []
        filtered_out_episodes = []

        for episode in episodes:
            # ä½¿ç”¨ä» config è¡¨è·å–çš„æ­£åˆ™è¡¨è¾¾å¼è¿›è¡Œè¿‡æ»¤
            match = blacklist_pattern.search(episode.title)
            if match:
                junk_type = match.group(0)
                filtered_out_episodes.append((episode, junk_type))
            else:
                filtered_episodes.append(episode)

        # æ‰“å°åˆ†é›†è¿‡æ»¤æ‘˜è¦
        summary_parts = [f"{self.provider_name}: åˆ†é›†è¿‡æ»¤ç»“æœ:"]

        # æ‰“å°è¿‡æ»¤æ‰çš„åˆ†é›†ï¼ˆè¿™äº›æ¯”è¾ƒé‡è¦ï¼Œé€æ¡åˆ—å‡ºï¼‰
        if filtered_out_episodes:
            summary_parts.append(f"  å·²è¿‡æ»¤ {len(filtered_out_episodes)} é›†:")
            for episode, junk_type in filtered_out_episodes:
                summary_parts.append(f"    âœ— {episode.title} ({junk_type})")

        # ä¿ç•™çš„åˆ†é›†åªæ˜¾ç¤ºæ•°é‡
        if filtered_episodes:
            summary_parts.append(f"  ä¿ç•™ {len(filtered_episodes)} é›†")

        if not filtered_episodes and not filtered_out_episodes:
            summary_parts.append(f"  æ— åˆ†é›†æ•°æ®")

        self.logger.info("\n".join(summary_parts))

        return filtered_episodes

    @abstractmethod
    async def close(self):
        """å…³é—­æ‰€æœ‰æ‰“å¼€çš„èµ„æºï¼Œä¾‹å¦‚HTTPå®¢æˆ·ç«¯ã€‚"""
        pass
